# 신경망에서 근사하다는 것은 무엇일까?

The term "approximate" in the context of neural networks means that the network tries to learn a function that is as close as possible to the actual, true function that describes the data.

When we say that "multiple non-linear layers can approximate a complex function," we're referring to the idea that a neural network (composed of many layers) can learn to represent or imitate a complex relationship between inputs and outputs in your data.

For example, consider a situation where you're trying to predict house prices based on several factors like size of the house, location, number of rooms, etc. The actual relationship between these factors and the house price is very complex - maybe even too complex for humans to perfectly describe with a simple mathematical equation.

However, a neural network with many layers can "approximate" this function. It can learn to mimic this complex relationship between inputs (house features) and outputs (house prices) by adjusting its weights and biases during training. The end result won't be a perfect match for the true, unknown function that describes house prices, but it's an "approximation" that is good enough to make useful predictions.

So, when training a neural network, the function you're trying to "approximate" is the complex, unknown function that describes the relationship in your data. The better your network can approximate this function, the more accurately it can make predictions based on new, unseen data.


요약해서 말하자면
복잡한 자연 환경에 복잡한 공식이 필요하지만
비선형 함수를 이용햐여 최대한 근사할 수 있다.